{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b058bba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word = PorterStemmer()\n",
    "word.stem(\"writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1deb5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word = PorterStemmer()\n",
    "l = []\n",
    "s = \"As we know that NLP is used to build applications such as sentiment analysis, QA systems, language translation, smart chatbots, voice systems, etc., hence, in order to build them, it becomes vital to understand the pattern in the text. The tokens, mentioned above, are very useful in finding and understanding these patterns. We can consider tokenization as the base step for other recipes such as stemming and lemmatization\"\n",
    "for i in s:\n",
    "    if word.stem(i) == True:\n",
    "        L.append(i)\n",
    "for i in range(len(l)):\n",
    "    print(l[i],end = \" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "156dd064",
   "metadata": {},
   "source": [
    "# The above one didnt work coz of the if statment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee207167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'we', 'know', 'that', 'NLP', 'is', 'used', 'to', 'build', 'applications', 'such', 'as', 'sentiment', 'analysis', ',', 'QA', 'systems', ',', 'language', 'translation', ',', 'smart', 'chatbots', ',', 'voice', 'systems', ',', 'etc.', ',', 'hence', ',', 'in', 'order', 'to', 'build', 'them', ',', 'it', 'becomes', 'vital', 'to', 'understand', 'the', 'pattern', 'in', 'the', 'text', '.', 'The', 'tokens', ',', 'mentioned', 'above', ',', 'are', 'very', 'useful', 'in', 'finding', 'and', 'understanding', 'these', 'patterns', '.', 'We', 'can', 'consider', 'tokenization', 'as', 'the', 'base', 'step', 'for', 'other', 'recipes', 'such', 'as', 'stemming', 'and', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"As we know that NLP is used to build applications such as sentiment analysis, QA systems, language translation, smart chatbots, voice systems, etc., hence, in order to build them, it becomes vital to understand the pattern in the text. The tokens, mentioned above, are very useful in finding and understanding these patterns. We can consider tokenization as the base step for other recipes such as stemming and lemmatization\"\n",
    "l = word_tokenize(s)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb3a2e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as we know that nlp is use to build applic such as sentiment analysi , qa system , languag translat , smart chatbot , voic system , etc. , henc , in order to build them , it becom vital to understand the pattern in the text . the token , mention abov , are veri use in find and understand these pattern . we can consid token as the base step for other recip such as stem and lemmat "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"As we know that NLP is used to build applications such as sentiment analysis, QA systems, language translation, smart chatbots, voice systems, etc., hence, in order to build them, it becomes vital to understand the pattern in the text. The tokens, mentioned above, are very useful in finding and understanding these patterns. We can consider tokenization as the base step for other recipes such as stemming and lemmatization\"\n",
    "l = word_tokenize(s)\n",
    "L = []\n",
    "for i in range(len(l)):\n",
    "    print(word.stem(l[i]),end = ' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d66cc731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World', 'Environment', 'Day', 'is', 'celebrated', 'every', 'year', 'on', '5th', 'June', '.', 'People', 'and', 'organizations', 'from', 'over', '143', 'countries', 'participate', 'in', 'the', 'events', '.', 'World', 'Environment', 'Day', 'is', 'observed', 'under', 'the', 'supervision', 'of', 'the', 'United', 'Nations', 'Environment', 'Programme', '(', 'UNEP', ')', 'since', 'the', 'year', '1974.The', 'main', 'idea', 'behind', 'the', 'observance', 'was', 'to', 'promote', 'environmental', 'conservation', '.', 'It', 'deals', 'with', 'all', 'the', 'constituents', 'of', 'the', 'environment', ',', 'that', 'is', ',', 'forests', ',', 'marine', 'life', ',', 'air', 'quality', ',', 'soil', 'pollution', ',', 'etc.On', 'World', 'Environment', 'Day', ',', 'people', 'are', 'encouraged', 'to', 'join', 'in', 'the', 'campaigns', 'and', 'make', 'efforts', 'for', 'environmental', 'protection', 'in', 'whatever', 'way', 'they', 'can', '.', 'They', 'are', 'encouraged', 'to', 'take', 'part', 'in', 'activities', 'that', 'promote', 'environmental', 'protection', '.', 'People', 'plant', 'young', 'saplings', 'on', 'their', 'neighborhood', ',', 'clean', 'the', 'garbage', 'from', 'roads', 'and', 'vacant', 'plots', ',', 'etc', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"World Environment Day is celebrated every year on 5th June. People and organizations from over 143 countries participate in the events. World Environment Day is observed under the supervision of the United Nations Environment Programme (UNEP) since the year 1974.The main idea behind the observance was to promote environmental conservation. It deals with all the constituents of the environment, that is, forests, marine life, air quality, soil pollution, etc.On World Environment Day, people are encouraged to join in the campaigns and make efforts for environmental protection in whatever way they can. They are encouraged to take part in activities that promote environmental protection. People plant young saplings on their neighborhood, clean the garbage from roads and vacant plots, etc.\"\n",
    "l = word_tokenize(s)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c63bceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'we', 'know', 'that', 'NLP', 'is', 'used', 'to', 'build', 'applications', 'such', 'as', 'sentiment', 'analysis', ',', 'QA', 'systems', ',', 'language', 'translation', ',', 'smart', 'chatbots', ',', 'voice', 'systems', ',', 'etc.', ',', 'hence', ',', 'in', 'order', 'to', 'build', 'them', ',', 'it', 'becomes', 'vital', 'to', 'understand', 'the', 'pattern', 'in', 'the', 'text', '.', 'The', 'tokens', ',', 'mentioned', 'above', ',', 'are', 'very', 'useful', 'in', 'finding', 'and', 'understanding', 'these', 'patterns', '.', 'We', 'can', 'consider', 'tokenization', 'as', 'the', 'base', 'step', 'for', 'other', 'recipes', 'such', 'as', 'stemming', 'and', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "f = open(\"text_nlp.txt\",'r')\n",
    "\n",
    "l = word_tokenize(f.read())\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4724657f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As',\n",
       " 'we',\n",
       " 'know',\n",
       " 'that',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'used',\n",
       " 'to',\n",
       " 'build',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'QA',\n",
       " 'systems',\n",
       " ',',\n",
       " 'language',\n",
       " 'translation',\n",
       " ',',\n",
       " 'smart',\n",
       " 'chatbots',\n",
       " ',',\n",
       " 'voice',\n",
       " 'systems',\n",
       " ',',\n",
       " 'etc.',\n",
       " ',',\n",
       " 'hence',\n",
       " ',',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'build',\n",
       " 'them',\n",
       " ',',\n",
       " 'it',\n",
       " 'becomes',\n",
       " 'vital',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'pattern',\n",
       " 'in',\n",
       " 'the',\n",
       " 'text.',\n",
       " 'The',\n",
       " 'tokens',\n",
       " ',',\n",
       " 'mentioned',\n",
       " 'above',\n",
       " ',',\n",
       " 'are',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'in',\n",
       " 'finding',\n",
       " 'and',\n",
       " 'understanding',\n",
       " 'these',\n",
       " 'patterns.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'consider',\n",
       " 'tokenization',\n",
       " 'as',\n",
       " 'the',\n",
       " 'base',\n",
       " 'step',\n",
       " 'for',\n",
       " 'other',\n",
       " 'recipes',\n",
       " 'such',\n",
       " 'as',\n",
       " 'stemming',\n",
       " 'and',\n",
       " 'lemmatization']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "Tokenizer_wrd = TreebankWordTokenizer()\n",
    "Tokenizer_wrd.tokenize(\n",
    " ' As we know that NLP is used to build applications such as sentiment analysis, QA systems, language translation, smart chatbots, voice systems, etc., hence, in order to build them, it becomes vital to understand the pattern in the text. The tokens, mentioned above, are very useful in finding and understanding these patterns. We can consider tokenization as the base step for other recipes such as stemming and lemmatization')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5727f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As',\n",
       " \"'\",\n",
       " 's',\n",
       " 'we',\n",
       " 'know',\n",
       " 'that',\n",
       " 'NLP',\n",
       " 'is',\n",
       " \"'\",\n",
       " 's',\n",
       " 'used',\n",
       " 'to',\n",
       " 'build',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'QA',\n",
       " 'systems',\n",
       " ',',\n",
       " 'language',\n",
       " 'translation',\n",
       " ',',\n",
       " 'smart',\n",
       " 'chatbots',\n",
       " ',',\n",
       " 'voice',\n",
       " 'systems',\n",
       " ',',\n",
       " 'etc',\n",
       " '.,',\n",
       " 'hence',\n",
       " ',',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'build',\n",
       " 'them',\n",
       " ',',\n",
       " 'it',\n",
       " 'becomes',\n",
       " 'vital',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'pattern',\n",
       " 'in',\n",
       " 'the',\n",
       " 'text',\n",
       " '.',\n",
       " 'The',\n",
       " 'tokens',\n",
       " ',',\n",
       " 'mentioned',\n",
       " 'above',\n",
       " ',',\n",
       " 'are',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'in',\n",
       " 'finding',\n",
       " 'and',\n",
       " 'understanding',\n",
       " 'these',\n",
       " 'patterns',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'consider',\n",
       " 'tokenization',\n",
       " 'as',\n",
       " 'the',\n",
       " 'base',\n",
       " 'step',\n",
       " 'for',\n",
       " 'other',\n",
       " 'recipes',\n",
       " 'such',\n",
       " 'as',\n",
       " 'stemming',\n",
       " 'and',\n",
       " 'lemmatization']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"As's we know that NLP is's used to build applications such as sentiment analysis, QA systems, language translation, smart chatbots, voice systems, etc., hence, in order to build them, it becomes vital to understand the pattern in the text. The tokens, mentioned above, are very useful in finding and understanding these patterns. We can consider tokenization as the base step for other recipes such as stemming and lemmatization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
